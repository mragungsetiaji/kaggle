{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "credit-card-fraud-detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mragungsetiaji/kaggle/blob/master/fraud_detection/credit_card_fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nU5aTQOtTYJY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zZ_LxQtOT-F4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://storage.googleapis.com/mrdatabucket001/kaggle_credit_card_fraud_detection/creditcard.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GKt-I_9dUJBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "7e78116b-ef69-4b04-875f-6517924d9cbf"
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...         V21       V22       V23       V24  \\\n",
              "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
              "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
              "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
              "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
              "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
              "\n",
              "        V25       V26       V27       V28  Amount  Class  \n",
              "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "fGFJY-W4UWPN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBrCvPRKetvk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Credit card Fraud Detection"
      ]
    },
    {
      "metadata": {
        "id": "9y5kwTWye9RU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "52LiowPgfluq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Checking the target classes"
      ]
    },
    {
      "metadata": {
        "id": "LWIWjdvhfOrE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "272b8ce2-f34c-49d9-bb2c-a702a20d60f9"
      },
      "cell_type": "code",
      "source": [
        "count_classes = pd.value_counts(data['Class'], sort = True).sort_index()\n",
        "count_classes.plot(kind = \"bar\")\n",
        "\n",
        "plt.title(\"Fraud class histogram\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Frequency\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Frequency')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAETCAYAAADge6tNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDBtJUneGjSSpO8NGktTd/wfPdsKHs6aZdQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "E6VesYGXgioy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clearly the data is totally unbalanced!\n",
        "\n",
        "This is a clear example where using a typical accuracy score to evaluate our classification algorithm. For example, if we just used a majority class to assign values to all records, we will still be having a high accuracy, BUT WE WOULD BE CLASSIFYING ALL \"1'\" INCORRECTLY!\n",
        "\n",
        "There are several ways to approach this classification problem taking into consideration this unbalance,\n",
        "\n",
        "- Collect more data? Nice strategy but not applicable in this case\n",
        "- Changing the performance metric:\n",
        "  - Use the confusion matrix to calculate Precision, Recall\n",
        "  - F1score (weighted average of precision recall)\n",
        "  - Use Kappa - which is a classification accuracy normalized by the imbalance of the classes in the data\n",
        "  - ROC curves - calculates sensitivity/specificity ratio\n",
        "- Resampling the dataset\n",
        "  - Essentially this is a method that will process the data to have an approximate 50-50 ratio.\n",
        "  - One way to achieve this is by OVER-sampling, which is adding copies of the under-represented class (better when you have little data)\n",
        "  - Another is UNDER-sampling, which deletes instances from the over-represented class (better when he have lots of data)\n",
        "  \n",
        "## Approach\n",
        "1. We are not going to perform feature engineering in first instance. The dataset has been downgraded in order to contain 30 features (28 anonamised + time + amount).\n",
        "2. We will then compare what happens when using resampling and when not using it. We will test this approach using a simple logistic regression classifier.\n",
        "3. We will evaluate the models by using some of the performance metrics mentioned above.\n",
        "4. We will repeat the best resampling/not resampling method, by tuning the parametes in the logistic regression classifier.\n",
        "5. We will finally perform classification model using other classification algorithms\n",
        "\n",
        "## Setting our input and target variables + resampling.\n",
        "1. Normalising the amount column. The amount column is not line with the anonimised features.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IwTs0VcugOB7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "7d592edc-5ef4-44f1-f44b-93ba5466065c"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data['normAmount'] = StandardScaler().fit_transform(np.array(data['Amount']).reshape(-1, 1))\n",
        "data = data.drop([\"Time\", \"Amount\"], axis=1)\n",
        "data.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Class</th>\n",
              "      <th>normAmount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>0</td>\n",
              "      <td>0.244964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.342475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>0</td>\n",
              "      <td>1.160686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>0</td>\n",
              "      <td>0.140534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.073403</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9       V10     ...           V21       V22       V23  \\\n",
              "0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n",
              "1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n",
              "2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n",
              "3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n",
              "4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n",
              "\n",
              "        V24       V25       V26       V27       V28  Class  normAmount  \n",
              "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n",
              "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n",
              "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n",
              "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n",
              "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "jnEUM_6wDz_m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Assigning X and Y. No resampling.\n",
        "3. Resampling.\n",
        "   - As we mentioned earlier, there are several ways to resample skewed data. Apart from under and over sampling, there is a very popular approach called SMOTE (Synthetic Minority Over-Sampling Technique), which is a combination of oversampling and undersampling, but the oversampling approach is not by replicating minority class but constructing new minority class data instance via an algorithm.\n",
        "\n",
        "   - In this notebook, we will use traditional UNDER-sampling. I will probably try to implement SMOTE in future versions of the code, but for now I will use traditional undersamplig.\n",
        "\n",
        "   - The way we will under sample the dataset will be by creating a 50/50 ratio. This will be done by randomly selecting \"x\" amount of sample from the majority class, being \"x\" the total number of records with the minority class."
      ]
    },
    {
      "metadata": {
        "id": "ezL1a3tICSNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = data.loc[:, data.columns != 'Class']\n",
        "y = data.loc[:, data.columns == 'Class']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_NnIhMMQCZTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of data points in the minority class\n",
        "number_records_fraud = len(data[data.Class == 1])\n",
        "fraud_indices = np.array(data[data.Class == 1].index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CpyU_mCaFASQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Picking the indices of the normal classes\n",
        "normal_indices = data[data.Class == 0].index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zupg4sbEFfvH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
        "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace=False)\n",
        "random_normal_indices = np.array(random_normal_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JhvKHd5HHK3V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Appending the 2 indices\n",
        "under_sample_indices = np.concatenate([fraud_indices, random_normal_indices])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgbKKDM6HNso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Under sample dataset\n",
        "under_sample_data = data.iloc[under_sample_indices, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GqnH36tbIUgl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']\n",
        "y_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y2K_a51TIeBR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5f4a7f5e-56fd-42ad-a653-c70983f384bb"
      },
      "cell_type": "code",
      "source": [
        "# Showing ratio\n",
        "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\n",
        "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\n",
        "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of normal transactions:  0.5\n",
            "Percentage of fraud transactions:  0.5\n",
            "Total number of transactions in resampled data:  984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EuB_nWWtJIVG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Splitting data into train and test set. Cross validation will be used when calculating accuracies."
      ]
    },
    {
      "metadata": {
        "id": "Pn3sh0nZI7M9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "01baff1f-f8a5-4a9c-9b4f-54518cf47cde"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Whole dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n",
        "\n",
        "print(\"Number transactions train dataset: \", len(X_train))\n",
        "print(\"Number transactions test dataset: \", len(X_test))\n",
        "print(\"Total number of transactions: \", len(X_train)+len(X_test))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number transactions train dataset:  199364\n",
            "Number transactions test dataset:  85443\n",
            "Total number of transactions:  284807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yRXhPDPsJORc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c7feeb0c-f039-4170-f3b0-80dceb0764a4"
      },
      "cell_type": "code",
      "source": [
        "# Undersampled dataset\n",
        "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
        "                                                                                                   ,y_undersample\n",
        "                                                                                                   ,test_size = 0.3\n",
        "                                                                                                   ,random_state = 0)\n",
        "print(\"\")\n",
        "print(\"Number transactions train dataset: \", len(X_train_undersample))\n",
        "print(\"Number transactions test dataset: \", len(X_test_undersample))\n",
        "print(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number transactions train dataset:  688\n",
            "Number transactions test dataset:  296\n",
            "Total number of transactions:  984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gDedLDfFJd8N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C-jc6dlWJqML",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Logistic regression classifier - Undersampled data\n",
        "We are very interested in the recall score, because that is the metric that will help us try to capture the most fraudulent transactions. If you think how Accuracy, Precision and Recall work for a confusion matrix, recall would be the most interesting:\n",
        "\n",
        "- Accuracy = (TP+TN)/total\n",
        "- Precision = TP/(TP+FP)\n",
        "- Recall = TP/(TP+FN)\n",
        "\n",
        "As we know, due to the imbalacing of the data, many observations could be predicted as False Negatives, being, that we predict a normal transaction, but it is in fact a fraudulent one. Recall captures this.\n",
        "\n",
        "- Obviously, trying to increase recall, tends to come with a decrease of precision. However, in our case, if we predict that a transaction is fraudulent and turns out not to be, is not a massive problem compared to the opposite.\n",
        "- We could even apply a cost function when having FN and FP with different weights for each type of error, but let's leave that aside for now."
      ]
    },
    {
      "metadata": {
        "id": "7zYVQqJHMLmd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lwz_LmaRMedK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Very ad-hoc function to print K_fold_scores"
      ]
    },
    {
      "metadata": {
        "id": "azSOkP0gMVIQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def printing_Kfold_scores(x_train_data, y_train_data):\n",
        "    fold = KFold(10, shuffle=False, random_state=5) \n",
        "\n",
        "    # Different C parameters\n",
        "    c_param_range = [0.01,0.1,1,10,100]\n",
        "\n",
        "    results_table = pd.DataFrame(index = range(len(c_param_range), 2), \n",
        "                                 columns = ['C_parameter','Mean recall score'])\n",
        "    results_table['C_parameter'] = c_param_range\n",
        "\n",
        "    # the k-fold will give 2 lists: train_indices = indices[0], \n",
        "    # test_indices = indices[1]\n",
        "    j = 0\n",
        "    for c_param in c_param_range:\n",
        "        print('-------------------------------------------')\n",
        "        print('C parameter: ', c_param)\n",
        "        print('-------------------------------------------')\n",
        "        print('')\n",
        "\n",
        "        recall_accs = []\n",
        "        iteration = 0\n",
        "        for train_index, test_index in fold.split(x_train_data):\n",
        "\n",
        "            # Call the logistic regression model with a certain C parameter\n",
        "            lr = LogisticRegression(C = c_param, penalty = 'l2', solver='lbfgs')\n",
        "\n",
        "            # Use the training data to fit the model. In this case, we use \n",
        "            # the portion of the fold to train the model with indices[0]. \n",
        "            # We then predict on the portion assigned as \n",
        "            # the 'test cross validation' with indices[1]\n",
        "            lr.fit(x_train_data.iloc[train_index,:], \n",
        "                   y_train_data.iloc[train_index,:].values.ravel())\n",
        "\n",
        "            # Predict values using the test indices in the training data\n",
        "            y_pred_undersample = lr.predict(x_train_data.iloc[test_index,:].values)\n",
        "\n",
        "            # Calculate the recall score and append it to a list for \n",
        "            # recall scores representing the current c_parameter\n",
        "            recall_acc = recall_score(y_train_data.iloc[test_index,:].values,\n",
        "                                      y_pred_undersample)\n",
        "            recall_accs.append(recall_acc)\n",
        "            iteration += 1\n",
        "            print('Iteration ', iteration,': recall score = ', recall_acc)\n",
        "\n",
        "        # The mean value of those recall scores is the metric we want to \n",
        "        # save and get hold of.\n",
        "        results_table.loc[j,'Mean recall score'] = np.mean(recall_accs)\n",
        "        j += 1\n",
        "        print('')\n",
        "        print('Mean recall score ', np.mean(recall_accs))\n",
        "        print('')\n",
        "\n",
        "    best_c = results_table['Mean recall score'].max()\n",
        "    \n",
        "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
        "    print('*********************************************************************************')\n",
        "    print('Best model to choose from cross validation is with C parameter = ', best_c)\n",
        "    print('*********************************************************************************')\n",
        "    \n",
        "    return best_c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jYJT-5Q4NaLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1873
        },
        "outputId": "80fcf72a-cb3b-4583-d36d-addaf0ffd689"
      },
      "cell_type": "code",
      "source": [
        "best_c = printing_Kfold_scores(X_train_undersample, y_train_undersample)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "C parameter:  0.01\n",
            "-------------------------------------------\n",
            "\n",
            "Iteration  1 : recall score =  0.8461538461538461\n",
            "Iteration  2 : recall score =  0.7941176470588235\n",
            "Iteration  3 : recall score =  0.8717948717948718\n",
            "Iteration  4 : recall score =  0.8235294117647058\n",
            "Iteration  5 : recall score =  0.8571428571428571\n",
            "Iteration  6 : recall score =  0.9354838709677419\n",
            "Iteration  7 : recall score =  0.9696969696969697\n",
            "Iteration  8 : recall score =  0.8809523809523809\n",
            "Iteration  9 : recall score =  0.918918918918919\n",
            "Iteration  10 : recall score =  0.8214285714285714\n",
            "\n",
            "Mean recall score  0.8719219345879688\n",
            "\n",
            "-------------------------------------------\n",
            "C parameter:  0.1\n",
            "-------------------------------------------\n",
            "\n",
            "Iteration  1 : recall score =  0.8974358974358975\n",
            "Iteration  2 : recall score =  0.7941176470588235\n",
            "Iteration  3 : recall score =  0.8974358974358975\n",
            "Iteration  4 : recall score =  0.8235294117647058\n",
            "Iteration  5 : recall score =  0.8928571428571429\n",
            "Iteration  6 : recall score =  0.9354838709677419\n",
            "Iteration  7 : recall score =  0.9696969696969697\n",
            "Iteration  8 : recall score =  0.9047619047619048\n",
            "Iteration  9 : recall score =  0.918918918918919\n",
            "Iteration  10 : recall score =  0.8214285714285714\n",
            "\n",
            "Mean recall score  0.8855666232326573\n",
            "\n",
            "-------------------------------------------\n",
            "C parameter:  1\n",
            "-------------------------------------------\n",
            "\n",
            "Iteration  1 : recall score =  0.8717948717948718\n",
            "Iteration  2 : recall score =  0.7941176470588235\n",
            "Iteration  3 : recall score =  0.8974358974358975\n",
            "Iteration  4 : recall score =  0.8529411764705882\n",
            "Iteration  5 : recall score =  0.9285714285714286\n",
            "Iteration  6 : recall score =  0.967741935483871\n",
            "Iteration  7 : recall score =  0.9696969696969697\n",
            "Iteration  8 : recall score =  0.9047619047619048\n",
            "Iteration  9 : recall score =  0.918918918918919\n",
            "Iteration  10 : recall score =  0.8571428571428571\n",
            "\n",
            "Mean recall score  0.8963123607336131\n",
            "\n",
            "-------------------------------------------\n",
            "C parameter:  10\n",
            "-------------------------------------------\n",
            "\n",
            "Iteration  1 : recall score =  0.8717948717948718\n",
            "Iteration  2 : recall score =  0.8235294117647058\n",
            "Iteration  3 : recall score =  0.8974358974358975\n",
            "Iteration  4 : recall score =  0.8529411764705882\n",
            "Iteration  5 : recall score =  0.9285714285714286\n",
            "Iteration  6 : recall score =  0.967741935483871\n",
            "Iteration  7 : recall score =  0.9696969696969697\n",
            "Iteration  8 : recall score =  0.9047619047619048\n",
            "Iteration  9 : recall score =  0.918918918918919\n",
            "Iteration  10 : recall score =  0.8928571428571429\n",
            "\n",
            "Mean recall score  0.9028249657756298\n",
            "\n",
            "-------------------------------------------\n",
            "C parameter:  100\n",
            "-------------------------------------------\n",
            "\n",
            "Iteration  1 : recall score =  0.8717948717948718\n",
            "Iteration  2 : recall score =  0.8235294117647058\n",
            "Iteration  3 : recall score =  0.8974358974358975\n",
            "Iteration  4 : recall score =  0.8529411764705882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration  5 : recall score =  0.9285714285714286\n",
            "Iteration  6 : recall score =  0.967741935483871\n",
            "Iteration  7 : recall score =  0.9696969696969697\n",
            "Iteration  8 : recall score =  0.9047619047619048\n",
            "Iteration  9 : recall score =  0.918918918918919\n",
            "Iteration  10 : recall score =  0.8928571428571429\n",
            "\n",
            "Mean recall score  0.9028249657756298\n",
            "\n",
            "*********************************************************************************\n",
            "Best model to choose from cross validation is with C parameter =  0.9028249657756298\n",
            "*********************************************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "r1PtlEfucOSF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a function to plot a fancy confusion matrix"
      ]
    },
    {
      "metadata": {
        "id": "5qVMe6afNd66",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    \n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        1#print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NTxYxyIAdglz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Predictions on test set and plotting confusion matrix\n",
        "\n",
        "We have been talking about using the recall metric as our proxy of how effective our predictive model is. Even though recall is still the recall we want to calculate, just bear mind in mind that the undersampled data hasn't got a skewness towards a certain class, which doesn't make recall metric as critical."
      ]
    },
    {
      "metadata": {
        "id": "04TZUStidt3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "78897f58-861b-48db-ed8f-25bf9fd24741"
      },
      "cell_type": "code",
      "source": [
        "# Use this C_parameter to build the final model with the whole training dataset \n",
        "# and predict the classes in the test dataset\n",
        "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
        "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
        "y_pred_undersample = lr.predict(X_test_undersample.values)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test_undersample, y_pred_undersample)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "print(\"Recall metric in the testing dataset: \", \n",
        "      cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')\n",
        "plt.show()\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Recall metric in the testing dataset:  0.8979591836734694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAEmCAYAAADmw8JdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqv6Ot5WFvAEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XWlcG3_nOG1C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fvb5_n36d6pw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, the model is offering an 93.2% recall accuracy on the generalised unseen data (test set). Not a bad percentage to be the first try. However, recall this is a 93.2% recall accuracy measure on the undersampled test set.\n",
        "\n",
        "Being happy with this result, let's apply the model we fitted and test it on the whole data."
      ]
    },
    {
      "metadata": {
        "id": "Z5JW3dEzeFYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "f411b7f5-af12-4257-9c83-966a03b5ba02"
      },
      "cell_type": "code",
      "source": [
        "# Use this C_parameter to build the final model with the whole training dataset \n",
        "# and predict the classes in the test dataset\n",
        "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
        "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
        "y_pred = lr.predict(X_test.values)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "print(\"Recall metric in the testing dataset: \", \n",
        "      cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Recall metric in the testing dataset:  0.9047619047619048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAEmCAYAAADIhuPPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2A322vwSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "TKcVXG1eejJE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NEpI0AmHexcJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Still a very decent recall accuracy when applying it to a much larger and skewed dataset!\n",
        "We can start to be happy with how initial approach is working.\n",
        "\n",
        "## Plotting ROC curve and Precision-Recall curve.\n",
        "- I find precision-recall curve much more convenient in this case as our problems relies on the \"positive\" class being more interesting than the negative class, but as we have calculated the recall precision, I am not going to plot the precision recall curves yet.\n",
        "\n",
        "- AUC and ROC curve are also interesting to check if the model is also predicting as a whole correctly and not making many errors"
      ]
    },
    {
      "metadata": {
        "id": "mdaGsvwge04A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "c8b8bbcf-af50-42a9-9bd2-2a76d085217d"
      },
      "cell_type": "code",
      "source": [
        "# ROC CURVE\n",
        "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
        "y_pred_undersample_score = lr.fit(X_train_undersample,\n",
        "                                  y_train_undersample.values.ravel()).decision_function(X_test_undersample.values)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test_undersample.values.ravel(),y_pred_undersample_score)\n",
        "roc_auc = auc(fpr,tpr)\n",
        "\n",
        "# Plot ROC\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot([0,1],[0,1],'r--')\n",
        "plt.xlim([-0.1,1.0])\n",
        "plt.ylim([-0.1,1.01])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "qu1qup2FfFTp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q1MjEdItf0tf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An additional comment that would be interesting to do is to initialise multiple undersampled datasets and repeat the process in loop. Remember that, to create an undersample data, we randomly got records from the majority class. Even though this is a valid technique, is doesn't represent the real population, so it would be interesting to repeat the process with different undersample configurations and check if the previous chosen parameters are still the most effective. In the end, the idea is to use a wider random representation of the whole dataset and rely on the averaged best parameters."
      ]
    },
    {
      "metadata": {
        "id": "KnTEYUmAgNSL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Logistic regression classifier - Skewed data\n",
        "Having tested our previous approach, I find really interesting to test the same process on the skewed data. Our intuition is that skewness will introduce issues difficult to capture, and therefore, provide a less effective algorithm.\n",
        "\n",
        "- To be fair, taking into account the fact that the train and test datasets are substantially bigger than the undersampled ones, I believe a K-fold cross validation is necessary. I guess that by splitting the data with 60% in training set, 20% cross validation and 20% test should be enough... but let's take the same approach as before (no harm on this, it's just that K-fold is computationally more expensive)"
      ]
    },
    {
      "metadata": {
        "id": "mqbITZNff3Cy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1513
        },
        "outputId": "16fa624c-ec18-43bb-e796-e8477ac7f75e"
      },
      "cell_type": "code",
      "source": [
        "best_c = printing_Kfold_scores(X_train,y_train)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "C parameter:  0.01\n",
            "-------------------------------------------\n",
            "\n",
            "Iteration  1 : recall score =  0.5277777777777778\n",
            "Iteration  2 : recall score =  0.5483870967741935\n",
            "Iteration  3 : recall score =  0.6111111111111112\n",
            "Iteration  4 : recall score =  0.6216216216216216\n",
            "Iteration  5 : recall score =  0.6551724137931034\n",
            "Iteration  6 : recall score =  0.6774193548387096\n",
            "Iteration  7 : recall score =  0.6923076923076923\n",
            "Iteration  8 : recall score =  0.46153846153846156\n",
            "Iteration  9 : recall score =  0.4888888888888889\n",
            "Iteration  10 : recall score =  0.5428571428571428\n",
            "\n",
            "Mean recall score  0.5827081561508702\n",
            "\n",
            "-------------------------------------------\n",
            "C parameter:  0.1\n",
            "-------------------------------------------\n",
            "\n",
            "Iteration  1 : recall score =  0.5277777777777778\n",
            "Iteration  2 : recall score =  0.5806451612903226\n",
            "Iteration  3 : recall score =  0.6111111111111112\n",
            "Iteration  4 : recall score =  0.6486486486486487\n",
            "Iteration  5 : recall score =  0.6551724137931034\n",
            "Iteration  6 : recall score =  0.7096774193548387\n",
            "Iteration  7 : recall score =  0.717948717948718\n",
            "Iteration  8 : recall score =  0.5\n",
            "Iteration  9 : recall score =  0.5555555555555556\n",
            "Iteration  10 : recall score =  0.5714285714285714\n",
            "\n",
            "Mean recall score  0.6077965376908647\n",
            "\n",
            "-------------------------------------------\n",
            "C parameter:  1\n",
            "-------------------------------------------\n",
            "\n",
            "Iteration  1 : recall score =  0.5277777777777778\n",
            "Iteration  2 : recall score =  0.5806451612903226\n",
            "Iteration  3 : recall score =  0.6111111111111112\n",
            "Iteration  4 : recall score =  0.6486486486486487\n",
            "Iteration  5 : recall score =  0.6551724137931034\n",
            "Iteration  6 : recall score =  0.7096774193548387\n",
            "Iteration  7 : recall score =  0.717948717948718\n",
            "Iteration  8 : recall score =  0.5\n",
            "Iteration  9 : recall score =  0.5777777777777777\n",
            "Iteration  10 : recall score =  0.6\n",
            "\n",
            "Mean recall score  0.6128759027702297\n",
            "\n",
            "-------------------------------------------\n",
            "C parameter:  10\n",
            "-------------------------------------------\n",
            "\n",
            "Iteration  1 : recall score =  0.5277777777777778\n",
            "Iteration  2 : recall score =  0.5806451612903226\n",
            "Iteration  3 : recall score =  0.6111111111111112\n",
            "Iteration  4 : recall score =  0.6486486486486487\n",
            "Iteration  5 : recall score =  0.6896551724137931\n",
            "Iteration  6 : recall score =  0.7096774193548387\n",
            "Iteration  7 : recall score =  0.717948717948718\n",
            "Iteration  8 : recall score =  0.5\n",
            "Iteration  9 : recall score =  0.5777777777777777\n",
            "Iteration  10 : recall score =  0.6\n",
            "\n",
            "Mean recall score  0.6163241786322987\n",
            "\n",
            "-------------------------------------------\n",
            "C parameter:  100\n",
            "-------------------------------------------\n",
            "\n",
            "Iteration  1 : recall score =  0.5277777777777778\n",
            "Iteration  2 : recall score =  0.5806451612903226\n",
            "Iteration  3 : recall score =  0.6111111111111112\n",
            "Iteration  4 : recall score =  0.6486486486486487\n",
            "Iteration  5 : recall score =  0.6896551724137931\n",
            "Iteration  6 : recall score =  0.7096774193548387\n",
            "Iteration  7 : recall score =  0.717948717948718\n",
            "Iteration  8 : recall score =  0.5\n",
            "Iteration  9 : recall score =  0.5777777777777777\n",
            "Iteration  10 : recall score =  0.6\n",
            "\n",
            "Mean recall score  0.6163241786322987\n",
            "\n",
            "*********************************************************************************\n",
            "Best model to choose from cross validation is with C parameter =  0.6163241786322987\n",
            "*********************************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lOx1K4BBg3jB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "af774963-c0e6-45e4-b73c-c5cdc2301051"
      },
      "cell_type": "code",
      "source": [
        "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
        "# dataset\n",
        "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
        "lr.fit(X_train,y_train.values.ravel())\n",
        "y_pred_undersample = lr.predict(X_test.values)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test,y_pred_undersample)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Recall metric in the testing dataset:  0.6190476190476191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAEmCAYAAADIhuPPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "W4bNHl8Riznp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}